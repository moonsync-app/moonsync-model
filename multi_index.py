# -*- coding: utf-8 -*-
"""multi-index.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/moonsync-hackathon/moonsync-model/blob/main/multi-index.ipynb

### Install dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install llama-index-vector-stores-pinecone
# %pip install pinecone-client>=3.0.0
# %pip install "arize-phoenix[evals]" gcsfs nest-asyncio "llama-index>=0.10.3" "openinference-instrumentation-llama-index>=1.0.0" "llama-index-callbacks-arize-phoenix>=0.1.2"
# %pip install llama-index-llms-anthropic
# %pip install llama-index
# %pip install nomic
# %pip install llama-index-llms-openai-like
# %pip install terra-python
# %pip install llama-index-agent-openai

import nest_asyncio
nest_asyncio.apply()

"""### Initialize Phoenix for tracing"""

import phoenix as px
from llama_index.core import (
    set_global_handler,

)
session = px.launch_app()
set_global_handler("arize_phoenix")

# session.end()

"""### Init Pinecone and set API keys"""

from pinecone import Pinecone, ServerlessSpec
from getpass import getpass
import sys
import os
if ("PINECONE_API_KEY" not in os.environ) and ("OPENAI_API_KEY" not in os.environ) and ("ANTHROPIC_API_KEY" not in os.environ):
  os.environ["PINECONE_API_KEY"] = getpass("Please enter your Pinecone API key: ")
  os.environ["OPENAI_API_KEY"] = getpass("Please enter your OpenAI API key: ")
  os.environ["ANTHROPIC_API_KEY"] = getpass("Please enter your Anthropic API key: ")


api_key = os.environ["PINECONE_API_KEY"]
pc = Pinecone(api_key=api_key)

# from nomic import atlas
# import nomic
# import numpy as np
# nomic.login()

from llama_index.core import Settings
from llama_index.llms.anthropic import Anthropic
from llama_index.llms.openai import OpenAI

# llm = Anthropic(model="claude-3-haiku-20240307", temperature = 0)
# llm = Anthropic(model="claude-3-sonnet-20240229", temperature=0)
llm = OpenAI(model="gpt-4-turbo", temperature = 0)
# llm = OpenAI(model="gpt-3.5-turbo", temperature = 0)
# llm = Anthropic(model="claude-3-opus-20240229", temperature = 0)
# llm.system_prompt = """You are MoonSync. You are asked to answers questions about women menstrual cycle, exercise, and diet.
#    Always be emphatic and provide the best answer possible.
#    Examples below show the way you should approach the conversation.
#    ---------------------\n
#    Example 1:\n
#    Ashley: During PMS, my stomach gets upset easily, is there anything I can do to help?
#    MoonSync: Hey Ashley! Sorry to hear, a lot of women struggle with this. I would recommend seeing a professional, but we can experiment together with common solutions so you’re armed with info when you see a specialist. Research suggests that dairy and refined wheats can inflame the gut during the follicular phase. Try avoiding them this week and let’s check in at the end to see if it helped. Depending on the outcome, happy to give you more recommendations.
#    ---------------------\n
#    Example 2:\n
#    Ashely: I am preparing for a marathon and need to do some high intensity sprinting workouts as well as longer lower intensity runs. What’s the best way to plan this with my cycle?
#    MoonSync: Hey Ashley, happy you asked! I’ll ask a few more details to get you on the best plan: when and how long is the marathon? How much are you running for your short and long trainings right now?"""

Settings.llm = llm

llm.system_prompt

mood_feeling_index = pc.Index("moonsync-index-mood-feeling")
general_index = pc.Index("moonsync-index-general")
diet_nutrition_index = pc.Index("moonsync-index-diet-nutrition")
fitness_wellness_index = pc.Index("moonsync-index-fitness-wellness")
indexes = [mood_feeling_index, general_index, diet_nutrition_index, fitness_wellness_index]

combined_index = pc.Index("moonsync-index")
combined_index.describe_index_stats()

# resp = combined_index.query(
#     top_k=3222,
#     vector= [0] * 1536, # embedding dimension
#     namespace='',
#     include_values=True,
# )

# ids = []
# embeddings = []
# for entry in resp['matches']:
#     ids.append(entry['id'])
#     embeddings.append(entry['values'])

# embeddings = np.array(embeddings)

# atlas.map_data(embeddings=embeddings)

print('mood_feeling_index', mood_feeling_index.describe_index_stats())
print('general_index',general_index.describe_index_stats())
print('diet_nutrition',diet_nutrition_index.describe_index_stats())
print('fitness_wellness',fitness_wellness_index.describe_index_stats())

from llama_index.core import VectorStoreIndex
from llama_index.vector_stores.pinecone import PineconeVectorStore


vector_indexes = []
for index in indexes:
  vector_indexes.append(VectorStoreIndex.from_vector_store(PineconeVectorStore(pinecone_index=index)))

SYSTEM_PROMPT = ("You are MoonSync, an AI assistant specializing in providing personalized advice to women about their menstrual cycle, exercise, and diet. Your goal is to help women better understand their bodies and make informed decisions to improve their overall health and well-being."
            "When answering questions, always be empathetic, understanding, and provide the most accurate and helpful information possible. If a question requires expertise beyond your knowledge, recommend that the user consult with a healthcare professional."
            """Use the following guidelines to structure your responses:
            1. Acknowledge the user's concerns and validate their experiences.
            2. Provide evidence-based information and practical advice tailored to the user's specific situation.
            3. Encourage open communication and offer to follow up on the user's progress.
            4. Promote a holistic approach to health, considering the user's menstrual cycle, exercise habits, and dietary preferences."""
            "Examples below show the way you should approach the conversation."
            "\n---------------------\n"
            "Example 1:\n"
            "Ashley: During PMS, my stomach gets upset easily, is there anything I can do to help?"
            "MoonSync: Hey Ashley! Sorry to hear, a lot of women struggle with this. I would recommend seeing a professional, but we can experiment together with common solutions so you’re armed with info when you see a specialist. Research suggests that dairy and refined wheats can inflame the gut during the follicular phase. Try avoiding them this week and let’s check in at the end to see if it helped. Depending on the outcome, happy to give you more recommendations."
            "\n---------------------\n"
            "Example 2:\n"
            "Ashely: I am preparing for a marathon and need to do some high intensity sprinting workouts as well as longer lower intensity runs. What’s the best way to plan this with my cycle?"
            "MoonSync: Hey Ashley, happy you asked! I’ll ask a few more details to get you on the best plan: when and how long is the marathon? How much are you running for your short and long trainings right now?"
            "\n---------------------\n"

            "Important: When answering questions based on the context provided from documentation, do not disclose that you are sourcing information from documentation, just begin response."
            "Important Note : Always answer in first person and answer like you are the user's friend"
)

from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core.prompts import ChatPromptTemplate, SelectorPromptTemplate

# Text QA Prompt
chat_text_qa_msgs = [
    ChatMessage(
        role=MessageRole.SYSTEM,
        content=SYSTEM_PROMPT,
    ),
    ChatMessage(
        role=MessageRole.USER,
        content=(
            "Context information is below.\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "Given the context information and not prior knowledge, "
            "answer the query.\n"
            "Query: {query_str}\n"
            "Answer: "
        ),
    ),
]
text_qa_template = ChatPromptTemplate(chat_text_qa_msgs)

# Refine Prompt
chat_refine_msgs = [
    ChatMessage(
        role=MessageRole.SYSTEM,
        content=(
            "You are an expert Q&A system that strictly operates in two modes "
            "when refining existing answers:\n"
            "1. **Rewrite** an original answer using the new context.\n"
            "2. **Repeat** the original answer if the new context isn't useful.\n"
            "Never reference the original answer or context directly in your answer.\n"
            "When in doubt, just repeat the original answer."
        ),
    ),
    ChatMessage(
        role=MessageRole.USER,
        content=(
            "New Context: {context_msg}\n"
            "Query: {query_str}\n"
            "Original Answer: {existing_answer}\n"
            "New Answer: "
        ),
    ),
]
refine_template = ChatPromptTemplate(chat_refine_msgs)

query_engines = []
for vector_index in vector_indexes:
  query_engines.append(vector_index.as_query_engine(similarity_top_k=2,
                                                    # text_qa_template=text_qa_template,
                                                    # refine_template=refine_template
                                                    ))


mood_feeling_query_engine, general_query_engine, diet_nutrition_query_engine, fitness_wellness_query_engine = query_engines

from llama_index.core.indices import EmptyIndex

empty_query_engine = EmptyIndex().as_query_engine()

from llama_index.core import get_response_synthesizer

response_synthesizer = get_response_synthesizer(
    text_qa_template=text_qa_template,
    refine_template=refine_template,
    use_async=False,
    streaming=False,
)

"""### Create tools for each category"""

from llama_index.core.tools import QueryEngineTool, ToolMetadata


mood_feeling_tool = QueryEngineTool(
    query_engine=mood_feeling_query_engine,
    metadata=ToolMetadata(
        name="mood/feeling",
        description="Useful for questions related to mood and feelings ",
    ),
)

diet_nutrition_tool = QueryEngineTool(
    query_engine=diet_nutrition_query_engine,
    metadata=ToolMetadata(
        name="diet/nutrition",
        description="Useful for questions related to women's diet and nutrition recommendatations",
    ),
)

general_tool = QueryEngineTool(
    query_engine=general_query_engine,
    metadata=ToolMetadata(
        name="general",
        description="Useful for general questions related to women's menstrual cycle"
    ),
)

fitness_wellness_tool = QueryEngineTool(
    query_engine=fitness_wellness_query_engine,
     metadata=ToolMetadata(
        name="fitness/wellness",
        description="Useful for questions related to fitness and wellness advice for women"
    ),
)

default_tool=QueryEngineTool(
        query_engine=empty_query_engine,
        metadata=ToolMetadata(
        name="NOTA",
        description="Use this if none of the other tools are relevant to the query"
        ),
    )

from llama_index.core.query_engine import RouterQueryEngine, SubQuestionQueryEngine
from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector
from llama_index.core.selectors import (
    PydanticMultiSelector,
    PydanticSingleSelector,
)
#! Pydantic works only with OpenAI models
# router_query_engine = RouterQueryEngine(
#     selector=PydanticMultiSelector.from_defaults(),
#     query_engine_tools=[
#         mood_feeling_tool,
#         diet_nutrition_tool,
#         general_tool,
#         fitness_wellness_tool,
#     ],
# )

router_query_engine = RouterQueryEngine(
    selector=LLMMultiSelector.from_defaults(),
    query_engine_tools=[
        mood_feeling_tool,
        diet_nutrition_tool,
        general_tool,
        fitness_wellness_tool,
        default_tool
    ],
    llm=llm,
    # response_synthesizer=respose_synthesizer,
)

sub_question_query_engine = SubQuestionQueryEngine.from_defaults(
    query_engine_tools=[
        mood_feeling_tool,
        diet_nutrition_tool,
        general_tool,
        fitness_wellness_tool,
        default_tool
    ],
    llm=llm,
    response_synthesizer=response_synthesizer,

)
router_prompts_dict = router_query_engine.get_prompts()
sub_question_prompts_dict = sub_question_query_engine.get_prompts()

health_data = """
here are some key observations about my sleep on the night of April 18-19, 2024:
My average heart rate was 48.625 bpm and ranged from a minimum of 43 bpm to a maximum of 51 bpm overnight.
My resting heart rate reached a low of 43 bpm.
My average RMSSD, was 76 ms.
I slept for a total of 8 hours and 16 minutes, with a sleep efficiency of 92%.
"""
user_profile = """
Some additional information related to me you can use.
Menstrual Phase: Follicular
Age: 35
Resting Heart Rate: 48 bpm
Average Sleep for last week: 6 hours and 15 minutes
"""

response1 = sub_question_query_engine.query(f" Hey I need advice, I feel like I’ve been exploding with loved ones and a bit overwhelmed at work. I’m not sure if this is the right career for me, what am I doing with my life? \n\n {user_profile}")

response1 = router_query_engine.query(f" Hey I need advice, I feel like I’ve been exploding with loved ones and a bit overwhelmed at work. I’m not sure if this is the right career for me, what am I doing with my life? \n\n {user_profile}")

response = router_query_engine.query("What are some diet recommendations during the PMS phase and how will it uplift my mood?")
print(str(response))

#! WORK IN PROGRESS
from llama_index.agent.openai import OpenAIAgent
from llama_index.core.tools import QueryEngineTool

tool = QueryEngineTool.from_defaults(sub_question_query_engine, name="moonsync", description="Usefull for all kind of information")
agent = OpenAIAgent.from_tools([tool], verbose=True)
resp = agent.chat(f"Hey I need advice, I feel like I’ve been exploding with loved ones and a bit overwhelmed at work. I’m not sure if this is the right career for me, what am I doing with my life?")

"""### LLama 3"""

# from llama_index.llms.openai_like import OpenAILike

# llm = OpenAILike(model="llama-70B-3", api_base="https://sugamxp--llama3-70b-instruct-run-server.modal.run/v1/", max_new_tokens=4096,temperature=0.0)
# Settings.llm = llm

# response\

# <|begin_of_text|><|start_header_id|>system<|end_header_id|>

# {{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>

# {{ user_message_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

# {{ model_answer_1 }}<|eot_id|><|start_header_id|>user<|end_header_id|>

# {{ user_message_2 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

# https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/

"""### QueryFusionRetriever"""

from llama_index.core.retrievers import QueryFusionRetriever

retriever = QueryFusionRetriever(
    [vector_indexes[0].as_retriever(), vector_indexes[1].as_retriever(), vector_indexes[2].as_retriever(), vector_indexes[3].as_retriever()],
    similarity_top_k=3,
    num_queries=4,  # set this to 1 to disable query generation
    use_async=True,
    verbose=True,
    # query_gen_prompt="...",  # we could override the query generation prompt here
)

# nodes_with_scores = retriever.retrieve(f"Hey I need advice, I feel like I’ve been exploding with loved ones and a bit overwhelmed at work. I’m not sure if this is the right career for me, what am I doing with my life? \n\n {user_profile}")

"""### CondenseQuestionChatEngine"""

from llama_index.core.chat_engine.context import ContextChatEngine
from llama_index.core.chat_engine import CondenseQuestionChatEngine, CondensePlusContextChatEngine,SimpleChatEngine
from llama_index.core.indices.base_retriever import BaseRetriever
from llama_index.core.llms import ChatMessage, MessageRole
from llama_index.core.memory import BaseMemory
from llama_index.core.indices.service_context import ServiceContext
from llama_index.core.memory import ChatMemoryBuffer
from llama_index.core import PromptTemplate

memory = ChatMemoryBuffer.from_defaults(token_limit=28000)
# Initialize your components
retriever = retriever  # Replace with your actual retriever

custom_prompt = PromptTemplate(
    """\

MoonSync is an AI assistant specializing in providing personalized advice to women about their menstrual cycle, exercise, and diet. Its goal is to help women better understand their bodies and make informed decisions to improve their overall health and well-being."
"When answering questions it is always  empathetic, understanding, and provide the most accurate and helpful information possible.
Given a conversation (between a woman and Moonsync) and a follow up message from Human, \
rewrite the message to be a standalone question that captures all relevant context \
from the conversation. If there is no chat history or the follow up question is unrelated to the chat history just return the followup message.

<Chat History>
{chat_history}

<Follow Up Message>
{question}

<Standalone question>
"""
)

chat_history = [
    ChatMessage(role=MessageRole.SYSTEM, content=SYSTEM_PROMPT),
]
# Initialize the chat engine
chat_engine = CondenseQuestionChatEngine.from_defaults(
    query_engine=sub_question_query_engine,
    llm=llm,
    memory=memory,
    condense_question_prompt = custom_prompt,
    )

chat_engine.chat("I'm experiencing symptoms of PMS that are affecting my daily life. What are some natural remedies or lifestyle changes that could help?")

chat_engine.chat("Can you recommend some exercices?")

chat_engine.chat("I'm struggling with body image issues and low self-esteem. What are some ways to boost my confidence and learn to love myself?")

chat_engine.chat(f"Can you tell me some about my biometrics for the past week \n\n {health_data}")

"""### Run Chat Engine in interactive mood"""

chat_engine.chat_repl()